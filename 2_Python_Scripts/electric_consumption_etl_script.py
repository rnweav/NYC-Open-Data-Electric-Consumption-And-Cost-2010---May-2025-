# -*- coding: utf-8 -*-
"""Electric_Consumption_ETL_Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ChzsAR856fKzlPXWzRutNZHITe1Q435p
"""

#Authorization to authenticate user for Google

from google.colab import auth
auth.authenticate_user()

#Import required libraries
import requests
import pandas as pd
from google.cloud import storage
from datetime import datetime

#Create variables for use
url = "https://data.cityofnewyork.us/resource/jr24-e7cr.json"
bucket_name="rweaver-electric-consumption-data"
folder_name="electric_consumption/"

data = []
offset = 0

#Create while loop to collect all pages of data from NYC Open Data site via API
while True:
    r = requests.get(url, params={"$limit": 50000, "$offset": offset})
    batch = r.json()
    if not batch:
        break
    data.extend(batch)
    offset += 50000

#Create dataframe with data retrieved
df=pd.DataFrame(data)

#Script to store dataframe in GCS

client=storage.Client()
bucket= client.bucket(bucket_name)
filename=f"electric_consumption_{datetime.now().strftime('%Y%m%d')}.csv"
blob=bucket.blob(filename)

df.to_csv(filename,index=False)
blob.upload_from_filename(filename)

import pandas as pd
from google.cloud import bigquery
from google.cloud import storage
import io

# Configuration
PROJECT_ID = "fabled-ranger-421314"
DATASET_ID = "electric_consumption"
BUCKET_NAME = "rweaver-electric-consumption-data"
RAW_FILE = "electric_consumption_20251202.csv"

# Initialize clients
bq_client = bigquery.Client(project=PROJECT_ID)
storage_client = storage.Client(project=PROJECT_ID)
bucket = storage_client.bucket(BUCKET_NAME)

# Load raw data from GCS
blob = bucket.blob(RAW_FILE)
csv_data = blob.download_as_text()
df_staging = pd.read_csv(io.StringIO(csv_data))

# Create staging table in BigQuery
table_id = f"{PROJECT_ID}.{DATASET_ID}.staging_electric_consumption"
job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
job = bq_client.load_table_from_dataframe(df_staging, table_id, job_config=job_config)
job.result()

print(f"Staging table created: {len(df_staging)} records")

#Get info on staging table
df.info()

df_staging.shape

df_staging.columns



"""### Data Cleaning: Remove Duplicates



"""

df=df_staging
#Check for duplicates
duplicates = df.duplicated(subset=['umis_bill_id']).sum()
print(f"\nDuplicate records (by UMIS BILL ID): {duplicates}")

#Remove duplicates
df=df.drop_duplicates(subset=['umis_bill_id'])

#Check for removal
print(df.duplicated(subset=['umis_bill_id']).sum())

"""### Data Cleaning: Remove Nulls"""

#Check for nulls
df.isnull().sum()

df = df.dropna(subset=['umis_bill_id', 'tds', 'meter_number', 'vendor_name'])

"""### Data Cleaning: Convert Numeric Columns"""

numeric_cols = ['consumption_kwh', 'consumption_kw', 'current_charges', 'kwh_charges', 'kw_charges', 'other_charges', 'days']
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

"""### Data Cleaning: Format Dates"""

print(f"\nSample Revenue Month values: {df['revenue_month'].head(3).tolist()}")
print(f"Sample Service Start Date: {df['service_start_date'].head(3).tolist()}")

#Convert format for service start and end dates
df['service_start_date'] = pd.to_datetime(df['service_start_date'], errors='coerce').dt.strftime('%Y-%m-%d')
df['service_end_date'] = pd.to_datetime(df['service_end_date'], errors='coerce').dt.strftime('%Y-%m-%d')

# Add date parts for service dates
df['service_start_year'] = pd.to_datetime(df['service_start_date'], errors='coerce').dt.year
df['service_start_month'] = pd.to_datetime(df['service_start_date'], errors='coerce').dt.month
df['service_start_day'] = pd.to_datetime(df['service_start_date'], errors='coerce').dt.day
df['service_start_quarter'] = pd.to_datetime(df['service_start_date'], errors='coerce').dt.quarter

df['service_end_year'] = pd.to_datetime(df['service_end_date'], errors='coerce').dt.year
df['service_end_month'] = pd.to_datetime(df['service_end_date'], errors='coerce').dt.month
df['service_end_day'] = pd.to_datetime(df['service_end_date'], errors='coerce').dt.day
df['service_end_quarter'] = pd.to_datetime(df['service_end_date'], errors='coerce').dt.quarter

cleaned_dates=[]

for date_string in df['revenue_month'].unique():
  date_obj=pd.to_datetime(str(date_string) + "-01")
  cleaned_dates.append({
      'revenue_month': date_string,
      'year' : date_obj.year,
      'quarter':f"Q{(date_obj.month-1)//3 + 1}",
      'month': date_obj.month,
      'month_name': date_obj.strftime('%B')
  })

"""### Data Prep: Create tables"""

#Date Dimension table
dim_date = pd.DataFrame(cleaned_dates).drop_duplicates()

# Vendor Dimension table
dim_vendor = df[['vendor_name']].drop_duplicates().reset_index(drop=True)

# Meter Dimension table
dim_meter = df[['meter_number', 'meter_scope', 'meter_amr', 'account_name']].drop_duplicates().reset_index(drop=True)

# Building Dimension table
dim_building = df[['tds', 'location', 'development_name', 'borough', 'amp', 'funding_source']].drop_duplicates().reset_index(drop=True)
dim_building.columns = ['tds_number', 'location', 'development_name', 'borough', 'amp_number', 'funding_source']

# Create fact table
fact = pd.DataFrame({
    'umis_bill_id': df['umis_bill_id'],
    'vendor_name': df['vendor_name'],
    'meter_number': df['meter_number'],
    'tds_number': df['tds'],
    'revenue_month': df['revenue_month'],
    'consumption_kwh': df['consumption_kwh'],
    'consumption_kw': df['consumption_kw'],
    'current_charges': df['current_charges'],
    'kwh_charges': df['kwh_charges'],
    'kw_charges': df['kw_charges'],
    'other_charges': df['other_charges'],
    'number_days': df['days'],
    'estimated_flag': df['estimated'].str.upper() == 'YES',
    'bill_analyzed_flag': df['bill_analyzed'].str.upper() == 'YES',
    'service_start_date': df['service_start_date'],
    'service_end_date': df['service_end_date'],
    'service_start_year': df['service_start_year'],
    'service_start_month': df['service_start_month'],
    'service_start_day': df['service_start_day'],
    'service_start_quarter': df['service_start_quarter'],
    'service_end_year': df['service_end_year'],
    'service_end_month': df['service_end_month'],
    'service_end_day': df['service_end_day'],
    'service_end_quarter': df['service_end_quarter'],
    'rate_class': df['rate_class']
})

#Save to Google Cloud Storage

timestamp = datetime.now().strftime('%Y%m%d')
for name, table in [('dim_vendor', dim_vendor), ('dim_meter', dim_meter),
                     ('dim_building', dim_building), ('dim_date', dim_date),
                     ('fact', fact)]:
    filename = f"transformed/{name}_{timestamp}.csv"
    table.to_csv(f"/tmp/{name}.csv", index=False)
    bucket.blob(filename).upload_from_filename(f"/tmp/{name}.csv")
    print(f"{name}: {len(table)} records")

print("Done!")

"""#Loading

###
"""

import pandas as pd
from google.cloud import bigquery
from google.cloud import storage
import io
from datetime import datetime

PROJECT_ID = "fabled-ranger-421314"
DATASET_ID = "electric_consumption"
BUCKET_NAME = "rweaver-electric-consumption-data"
TIMESTAMP = datetime.now().strftime('%Y%m%d')

bq_client = bigquery.Client(project=PROJECT_ID)
storage_client = storage.Client(project=PROJECT_ID)
bucket = storage_client.bucket(BUCKET_NAME)

# Load dim_vendor
blob = bucket.blob(f"transformed/dim_vendor_{TIMESTAMP}.csv")
csv_data = blob.download_as_text()
dim_vendor = pd.read_csv(io.StringIO(csv_data))
dim_vendor['vendor_key'] = range(1, len(dim_vendor) + 1)

table_id = f"{PROJECT_ID}.{DATASET_ID}.dim_vendor"
job = bq_client.load_table_from_dataframe(dim_vendor, table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
job.result()

# Load dim_meter
blob = bucket.blob(f"transformed/dim_meter_{TIMESTAMP}.csv")
csv_data = blob.download_as_text()
dim_meter = pd.read_csv(io.StringIO(csv_data))
dim_meter['meter_key'] = range(1, len(dim_meter) + 1)

table_id = f"{PROJECT_ID}.{DATASET_ID}.dim_meter"
job = bq_client.load_table_from_dataframe(dim_meter, table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
job.result()

# Load dim_building
blob = bucket.blob(f"transformed/dim_building_{TIMESTAMP}.csv")
csv_data = blob.download_as_text()
dim_building = pd.read_csv(io.StringIO(csv_data))
dim_building['building_key'] = range(1, len(dim_building) + 1)

table_id = f"{PROJECT_ID}.{DATASET_ID}.dim_building"
job = bq_client.load_table_from_dataframe(dim_building, table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
job.result()

# Load dim_date
blob = bucket.blob(f"transformed/dim_date_{TIMESTAMP}.csv")
csv_data = blob.download_as_text()
dim_date = pd.read_csv(io.StringIO(csv_data))
dim_date['date_key'] = range(1, len(dim_date) + 1)

table_id = f"{PROJECT_ID}.{DATASET_ID}.dim_date"
job = bq_client.load_table_from_dataframe(dim_date, table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
job.result()

# Load fact table with foreign keys
blob = bucket.blob(f"transformed/fact_{TIMESTAMP}.csv")
csv_data = blob.download_as_text()
fact = pd.read_csv(io.StringIO(csv_data))

# Merge to get foreign keys
fact = fact.merge(dim_vendor[['vendor_name', 'vendor_key']], on='vendor_name', how='left')
fact = fact.merge(dim_meter[['meter_number', 'meter_key']], on='meter_number', how='left')
fact = fact.merge(dim_building[['tds_number', 'building_key']], on='tds_number', how='left')
fact = fact.merge(dim_date[['revenue_month', 'date_key']], on='revenue_month', how='left')

# Add surrogate key
fact['consumption_key'] = range(1, len(fact) + 1)

# Select final columns
fact_final = fact[[
    'consumption_key',
    'building_key',
    'date_key',
    'meter_key',
    'vendor_key',
    'umis_bill_id',
    'consumption_kwh',
    'consumption_kw',
    'current_charges',
    'kwh_charges',
    'kw_charges',
    'other_charges',
    'number_days',
    'estimated_flag',
    'bill_analyzed_flag',
    'service_start_date',
    'service_end_date',
    'service_start_year',
    'service_start_month',
    'service_start_day',
    'service_start_quarter',
    'service_end_year',
    'service_end_month',
    'service_end_day',
    'service_end_quarter',
    'rate_class'
]]

# Remove duplicates
fact_final = fact_final.drop_duplicates(subset=['umis_bill_id'])

# Reassign surrogate keys after deduplication
fact_final['consumption_key'] = range(1, len(fact_final) + 1)

table_id = f"{PROJECT_ID}.{DATASET_ID}.fact_electric_consumption"
job = bq_client.load_table_from_dataframe(fact_final, table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE"))
job.result()

print("Done")